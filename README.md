# ğŸš€ AdaBoost From Scratch (Work in Progress)

This repository contains my implementation of the **AdaBoost** algorithm built **from scratch in Python**, without relying on pre-built machine learning libraries for the core logic.  

The goal is to deeply understand the boosting process by manually coding:
- Weak learners (decision stumps)
- Weight updates
- Error calculation
- Final strong classifier construction

---

## ğŸ“Œ Status
âš ï¸ **Work in Progress** â€” core algorithm is partially complete, with ongoing improvements to:
- Feature handling
- Margin analysis
- Visualization of training process
- Performance comparison with `scikit-learn`'s AdaBoost

---

## ğŸ¯ Features (Planned & Implemented)
- âœ… Decision stump implementation
- âœ… Weighted error computation
- âœ… Alpha (learner weight) calculation
- âœ… Data weight updates per boosting round
- â³ Multi-feature handling
- â³ ROC curve and AUC score visualization
- â³ Margin distribution analysis

---

## ğŸ“‚ Repository Structure
