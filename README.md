# 🚀 AdaBoost From Scratch (Work in Progress)

This repository contains my implementation of the **AdaBoost** algorithm built **from scratch in Python**, without relying on pre-built machine learning libraries for the core logic.  

The goal is to deeply understand the boosting process by manually coding:
- Weak learners (decision stumps)
- Weight updates
- Error calculation
- Final strong classifier construction

---

## 📌 Status
⚠️ **Work in Progress** — core algorithm is partially complete, with ongoing improvements to:
- Feature handling
- Margin analysis
- Visualization of training process
- Performance comparison with `scikit-learn`'s AdaBoost

---

## 🎯 Features (Planned & Implemented)
- ✅ Decision stump implementation
- ✅ Weighted error computation
- ✅ Alpha (learner weight) calculation
- ✅ Data weight updates per boosting round
- ⏳ Multi-feature handling
- ⏳ ROC curve and AUC score visualization
- ⏳ Margin distribution analysis

---

## 📂 Repository Structure
